{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309938bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in “London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall reviews, privates from price, dorms from price, facilities and property description.\n",
    " \n",
    "    \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_hostels_in_london():\n",
    "    url = \"https://www.hostelworld.com/search?search_keywords=London,%20England&country=England&city=London&date_from=2023-06-01&date_to=2023-06-30&number_of_guests=1\"\n",
    "    \n",
    "\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch page: {response.status_code}\")\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "   \n",
    "    hostels = soup.find_all('div', class_='fabresult')\n",
    "    \n",
    "    hostels_list = []\n",
    "    \n",
    "    for hostel in hostels:\n",
    "        details = {}\n",
    "        \n",
    "      \n",
    "        name = hostel.find('h2', class_='title')\n",
    "        details['Hostel Name'] = name.text.strip() if name else \"-\"\n",
    "        \n",
    "      \n",
    "        distance = hostel.find('span', class_='description')\n",
    "        details['Distance from City Centre'] = distance.text.strip() if distance else \"-\"\n",
    "        \n",
    "        \n",
    "        ratings = hostel.find('div', class_='score orange big')\n",
    "        details['Ratings'] = ratings.text.strip() if ratings else \"-\"\n",
    "        \n",
    "\n",
    "        total_reviews = hostel.find('div', class_='reviews')\n",
    "        details['Total Reviews'] = total_reviews.text.strip() if total_reviews else \"-\"\n",
    "        \n",
    "       \n",
    "        overall_reviews = hostel.find('div', class_='keyword')\n",
    "        details['Overall Reviews'] = overall_reviews.text.strip() if overall_reviews else \"-\"\n",
    "        \n",
    "       \n",
    "        prices = hostel.find('div', class_='price-col')\n",
    "        if prices:\n",
    "            privates_price = prices.find('span', class_='price')\n",
    "            details['Privates from Price'] = privates_price.text.strip() if privates_price else \"-\"\n",
    "            \n",
    "            dorms_price = prices.find('span', class_='price')\n",
    "            details['Dorms from Price'] = dorms_price.text.strip() if dorms_price\n",
    "\n",
    "\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0e772a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted from any YouTube Video.\n",
    "\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def fetch_youtube_comments(api_key, video_id, max_results=500):\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    \n",
    "    comments = []\n",
    "    next_page_token = None\n",
    "    total_results = 0\n",
    "\n",
    "    while total_results < max_results:\n",
    "        try:\n",
    "            if next_page_token:\n",
    "                response = youtube.commentThreads().list(\n",
    "                    part='snippet',\n",
    "                    videoId=video_id,\n",
    "                    pageToken=next_page_token,\n",
    "                    maxResults=min(max_results - total_results, 100)\n",
    "                ).execute()\n",
    "            else:\n",
    "                response = youtube.commentThreads().list(\n",
    "                    part='snippet',\n",
    "                    videoId=video_id,\n",
    "                    maxResults=min(max_results, 100)\n",
    "                ).execute()\n",
    "\n",
    "            for item in response['items']:\n",
    "                comment = item['snippet']['topLevelComment']['snippet']\n",
    "                comments.append({\n",
    "                    'Comment': comment['textDisplay'],\n",
    "                    'Upvotes': comment['likeCount'],\n",
    "                    'Published At': comment['publishedAt']\n",
    "                })\n",
    "                total_results += 1\n",
    "\n",
    "            next_page_token = response.get('nextPageToken')\n",
    "\n",
    "            if not next_page_token:\n",
    "                break\n",
    "\n",
    "        except HttpError as e:\n",
    "            print(f\"Error fetching comments: {e}\")\n",
    "            break\n",
    "\n",
    "    return comments\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    api_key = 'your_api_key_here'\n",
    "    video_id = 'your_video_id_here'\n",
    "\n",
    " \n",
    "    comments_data = fetch_youtube_comments(api_key, video_id, max_results=500)\n",
    "\n",
    "    if comments_data:\n",
    "     \n",
    "        df = pd.DataFrame(comments_data)\n",
    "\n",
    "        \n",
    "        df.to_csv('youtube_comments.csv', index=False)\n",
    "\n",
    "        print(f\"Successfully fetched {len(df)} comments. Saved to 'youtube_comments.csv'\")\n",
    "    else:\n",
    "        print(\"Failed to fetch comments.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd10a8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped: “Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_forbes_billionaires():\n",
    "    url = \"https://www.forbes.com/billionaires/\"\n",
    "    \n",
    "   \n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch page: {response.status_code}\")\n",
    "        return None\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    \n",
    "    billionaires = soup.find_all('div', class_='person')\n",
    "    \n",
    "    billionaires_list = []\n",
    "    \n",
    "    for person in billionaires:\n",
    "        details = {}\n",
    "        \n",
    "       \n",
    "        rank = person.find('div', class_='rank')\n",
    "        details['Rank'] = rank.text.strip() if rank else \"-\"\n",
    "     \n",
    "        name = person.find('div', class_='personName')\n",
    "        details['Name'] = name.text.strip() if name else \"-\"\n",
    "        \n",
    "      \n",
    "        net_worth = person.find('div', class_='netWorth')\n",
    "        details['Net Worth'] = net_worth.text.strip() if net_worth else \"-\"\n",
    "        \n",
    "      \n",
    "        age = person.find('div', class_='age')\n",
    "        details['Age'] = age.text.strip() if age else \"-\"\n",
    "        \n",
    "      \n",
    "        citizenship = person.find('div', class_='countryOfCitizenship')\n",
    "        details['Citizenship'] = citizenship.text.strip() if citizenship else \"-\"\n",
    "        \n",
    "       \n",
    "        source = person.find('div', class_='source')\n",
    "        details['Source'] = source.text.strip() if source else \"-\"\n",
    "        \n",
    "       \n",
    "        industry = person.find('div', class_='category')\n",
    "        details['Industry'] = industry.text.strip() if industry else \"-\"\n",
    "        \n",
    "        billionaires_list.append(details)\n",
    "    \n",
    "    return billionaires_list\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "   \n",
    "    billionaires_data = scrape_forbes_billionaires()\n",
    "    \n",
    "    if billionaires_data:\n",
    "        \n",
    "        df = pd.DataFrame(billionaires_data)\n",
    "        \n",
    "      \n",
    "        df.to_csv('forbes_billionaires.csv', index=False)\n",
    "        \n",
    "        print(\"Scraping and saving complete.\")\n",
    "    else:\n",
    "        print(\"Failed to scrape Forbes billionaires data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed180964",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a program to scrap all the available details of best gaming laptops from digit.in. \n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_best_gaming_laptops():\n",
    "    url = \"https://www.digit.in/top-products/best-gaming-laptops-40.html\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch page: {response.status_code}\")\n",
    "        return None\n",
    "    \n",
    " \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    \n",
    "    laptop_containers = soup.find_all('div', class_='TopNumbeHeading active sticky-footer')\n",
    "    \n",
    "    laptops_list = []\n",
    "    \n",
    "    for laptop in laptop_containers:\n",
    "        details = {}\n",
    "        \n",
    "        name = laptop.find('div', class_='TopNumbeHeading active sticky-footer')\n",
    "        details['Name'] = name.text.strip()\n",
    "        \n",
    "   \n",
    "        specifications = laptop.find('div', class_='specs').text.strip()\n",
    "        details['Specifications'] = specifications\n",
    "        \n",
    "        \n",
    "        price = laptop.find('div', class_='Price')\n",
    "        details[' Price']= ' his\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f51c905",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps. \n",
    "\n",
    "import requests\n",
    "\n",
    "def get_geocode(city_name, api_key):\n",
    "    base_url = \"https://maps.googleapis.com/maps/api/geocode/json\"\n",
    "    params = {\n",
    "        \"address\": city_name,\n",
    "        \"key\": api_key\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        data = response.json()\n",
    "\n",
    "        if data['status'] == 'OK' and len(data['results']) > 0:\n",
    "            location = data['results'][0]['geometry']['location']\n",
    "            latitude = location['lat']\n",
    "            longitude = location['lng']\n",
    "            return latitude, longitude\n",
    "        else:\n",
    "            print(f\"No results found for {city_name}\")\n",
    "            return None, None\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching geocode data: {e}\")\n",
    "        return None, None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    city_name = input(\"Enter the city name to get geocode (latitude, longitude): \").strip()\n",
    "    api_key = \"your_api_key_here\" \n",
    "\n",
    "    latitude, longitude = get_geocode(city_name, api_key)\n",
    "\n",
    "    if latitude is not None and longitude is not None:\n",
    "        print(f\"Geospatial coordinates (latitude, longitude) for {city_name}:\")\n",
    "        print(f\"Latitude: {latitude}\")\n",
    "        print(f\"Longitude: {longitude}\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve geospatial coordinates for {city_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e22eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com and scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”, “Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the details is missing then replace it by “- “. Save your results in a dataframe and CSV.\n",
    "#To scrape smartphone details from the first page of search results on Flipkart.com, we'll use Python along with the `requests`, `BeautifulSoup`, and `pandas` libraries. Below is the complete Python program to achieve this:\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_flipkart_smartphones(search_query):\n",
    "    base_url = f\"https://www.flipkart.com/search?q={search_query.replace(' ', '+')}\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(base_url, headers=headers)\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        products = []\n",
    "        results = soup.find_all('div', {'class': '_1AtVbE'})\n",
    "        \n",
    "        for result in results:\n",
    "            product = {}\n",
    "            \n",
    "            try:\n",
    "                product['Brand'] = result.find('div', {'class': '_4rR01T'}).text.strip().split()[0]\n",
    "            except AttributeError:\n",
    "                product['Brand'] = \"-\"\n",
    "            \n",
    "            try:\n",
    "                product['Name'] = result.find('a', {'class': 'IRpwTa'}).text.strip()\n",
    "            except AttributeError:\n",
    "                product['Name'] = \"-\"\n",
    "            \n",
    "            try:\n",
    "                product['Color'] = result.find('a', {'class': 'IRpwTa'}).text.strip()\n",
    "            except AttributeError:\n",
    "                product['Color'] = \"-\"\n",
    "            \n",
    "            try:\n",
    "                product['RAM'] = result.find('ul', {'class': '_1xgFaf'}).find_all('li')[0].text.strip()\n",
    "            except AttributeError:\n",
    "                product['RAM'] = \"-\"\n",
    "            \n",
    "            try:\n",
    "                product['Storage(ROM)'] = result.find('ul', {'class': '_1xgFaf'}).find_all('li')[1].text.strip()\n",
    "            except AttributeError:\n",
    "                product['Storage(ROM)'] = \"-\"\n",
    "            \n",
    "            try:\n",
    "                product['Primary Camera'] = result.find('ul', {'class': '_1xgFaf'}).find_all('li')[2].text.strip()\n",
    "            except AttributeError:\n",
    "                product['Primary Camera'] = \"-\"\n",
    "            \n",
    "            try:\n",
    "                product['Secondary Camera'] = result.find('ul', {'class': '_1xgFaf'}).find_all('li')[3].text.strip()\n",
    "            except AttributeError:\n",
    "                product['Secondary Camera'] = \"-\"\n",
    "            \n",
    "            try:\n",
    "                product['Display Size'] = result.find('ul', {'class': '_1xgFaf'}).find_all('li')[4].text.strip()\n",
    "            except AttributeError:\n",
    "                product['Display Size'] = \"-\"\n",
    "            \n",
    "            try:\n",
    "                product['Battery Capacity'] = result.find('ul', {'class': '_1xgFaf'}).find_all('li')[5].text.strip()\n",
    "            except AttributeError:\n",
    "                product['Battery Capacity'] = \"-\"\n",
    "            \n",
    "            try:\n",
    "                product['Price'] = result.find('div', {'class': '_30jeq3 _1_WHN1'}).text.strip().replace('₹','Rs')\n",
    "            except AttributeError:\n",
    "                Sale product\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67458d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a python program to access the search bar and search button on images.google.com and scrape 10 images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’.\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "import hashlib\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Function to scroll and load more images\n",
    "def scroll_and_load(driver, timeout):\n",
    "    # Scroll down to the bottom of the page\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(timeout)\n",
    "\n",
    "# Function to save image from URL\n",
    "def save_image(url, directory):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            # Generate a unique filename using MD5 hash of the URL\n",
    "            filename = os.path.join(directory, hashlib.md5(url.encode()).hexdigest() + '.jpg')\n",
    "            with open(filename, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            print(f\"Image saved: {filename}\")\n",
    "        else:\n",
    "            print(f\"Failed to retrieve image from URL: {url}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Exception occurred while downloading image: {e}\")\n",
    "\n",
    "def scrape_images(keyword, num_images=10):\n",
    "   \n",
    "    driver = webdriver.Chrome(executable_path='your/path/to/chromedriver.exe')\n",
    "\n",
    "    \n",
    "    driver.get('https://images.google.com/')\n",
    "    \n",
    "\n",
    "    search_bar = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.NAME, 'q')))\n",
    "    search_bar.send_keys(keyword)\n",
    "    search_bar.send_keys(Keys.RETURN)\n",
    "    \n",
    "\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CLASS_NAME, 'rg_i')))\n",
    "\n",
    "   \n",
    "    num_scrolls = num_images // 20  \n",
    "    for _ in range(num_scrolls):\n",
    "        scroll_and_load(driver, 2)  \n",
    "\n",
    "    \n",
    "    image_elements = driver.find_elements_by_class_name('rg_i')\n",
    "\n",
    "    \n",
    "    image_urls = []\n",
    "    for img in image_elements[:num_images]:\n",
    "        try:\n",
    "            img_url = img.get_attribute('src')\n",
    "            if img_url.startswith('http'):\n",
    "                image_urls.append(img_url)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching image URL: {e}\")\n",
    "\n",
    "\n",
    "    save_directory = './images'\n",
    "    if not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory)\n",
    "\n",
    "    for idx, url in enumerate(image_urls):\n",
    "        save_image(url, save_directory)\n",
    "\n",
    "    \n",
    "    driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    keywords = ['fruits', 'cars', 'Machine Learning', 'Guitar', 'Cakes']\n",
    "    for keyword in keywords:\n",
    "        print(f\"Scraping images for '{keyword}'...\")\n",
    "        scrape_images(keyword, num_images=10)\n",
    "        print(f\"Finished scraping images for '{keyword}'\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3886bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#In the above question, now scrape the following details of each product listed in first 3 pages of your search results and save it in a data frame and csv. In case if any product has less than 3 pages in search results then scrape all the products available under that product name. Details to be scraped are: \"Brand Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\" and “Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“.\n",
    "\n",
    "#To extend the previous Python program to scrape multiple pages of search results from Amazon.in, and to save the data into a dataframe and CSV file, we need to enhance our scraping logic. We'll iterate through multiple pages of search results and collect additional details for each product. Here's the updated program:\n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_product_details(product_name, num_pages=3):\n",
    "    base_url = f\"https://www.amazon.in/s?k={product_name.replace(' ', '+')}\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    \n",
    "    products = []\n",
    "    \n",
    "    for page in range(1, num_pages + 1):\n",
    "        url = f\"{base_url}&page={page}\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            response.raise_for_status()  \n",
    "\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            results = soup.find_all('div', {'data-component-type': 's-search-result'})\n",
    "            \n",
    "            for result in results:\n",
    "                product = {}\n",
    "                \n",
    "                \n",
    "                try:\n",
    "                    product['Brand'] = result.find('span', {'class': 'a-size-base-plus'}).text.strip()\n",
    "                except AttributeError:\n",
    "                    product['Brand'] = \"\"\n",
    "                \n",
    "                try:\n",
    "                    product['Name'] = result.find('span', {'class': 'a-size-medium'}).text.strip()\n",
    "                except AttributeError:\n",
    "                    product['Name'] = \"\"\n",
    "                \n",
    "                try:\n",
    "                    product['Price'] = result.find('span', {'class': 'a-price'}).find('span', {'class': 'a-offscreen'}).text.strip()\n",
    "                except AttributeError:\n",
    "                    product['Price'] = \"\"\n",
    "                \n",
    "                try:\n",
    "                    product['Return/Exchange'] = result.find('span', {'class': 'a-text-bold'}).text.strip()\n",
    "                except AttributeError:\n",
    "                    product['Return/Exchange'] = \"\"\n",
    "                \n",
    "                try:\n",
    "                    product['Expected Delivery'] = result.find('span', {'class': 'a-text-bold'}).text.strip()\n",
    "                except AttributeError:\n",
    "                    product['Expected Delivery'] = \"\"\n",
    "                \n",
    "                try:\n",
    "                    product['Availability'] = result.find('span', {'class': 'a-size-small'}).text.strip()\n",
    "                except AttributeError:\n",
    "                    product['Availability'] = \"\"\n",
    "                \n",
    "                try:\n",
    "                    product['URL'] = 'https://www.amazon.in' + result.find('a', {'class': 'a-link-normal'})['href']\n",
    "                except TypeError:\n",
    "                    modified their So from colleagues had\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91218a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a python program which searches all the product under a particular product from www.amazon.in. The product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search for guitars\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def search_amazon_product(product_name):\n",
    "    base_url = f\"https://www.amazon.in/s?k={product_name.replace(' ', '+')}\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(base_url, headers=headers)\n",
    "        response.raise_for_status()  \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        products = []\n",
    "        results = soup.find_all('div', {'data-component-type': 's-search-result'})\n",
    "        \n",
    "        for result in results:\n",
    "            product = {\n",
    "                'title': result.find('span', {'class': 'a-size-medium'}).text.strip(),\n",
    "                'price': result.find('span', {'class': 'a-price'}).find('span', {'class': 'a-offscreen'}).text.strip(),\n",
    "                'rating': result.find('span', {'class': 'a-icon-alt'}).text.strip(),\n",
    "                'url': 'https://www.amazon.in' + result.find('a', {'class': 'a-link-normal'})['href']\n",
    "            }\n",
    "            products.append(product)\n",
    "        \n",
    "        return products\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching Amazon results: {e}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    product_name = input(\"Enter the product you want to search on Amazon.in: \")\n",
    "    products = search_amazon_product(product_name)\n",
    "    \n",
    "    if products:\n",
    "        print(f\"Found {len(products)} products matching '{product_name}' on Amazon.in:\")\n",
    "        for idx, product in enumerate(products, start=1):\n",
    "            print(f\"{idx}. {product['title']} - Price: {product['price']} - Rating: {product['rating']} - URL: {product['url']}\")\n",
    "    else:\n",
    "        print(f\"No products found for '{product_name}' on Amazon.in\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
